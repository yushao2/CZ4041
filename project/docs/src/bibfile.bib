@article{ILSVRC15,
	Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
	Title = {{ImageNet Large Scale Visual Recognition Challenge}},
	Year = {2015},
	journal   = {International Journal of Computer Vision (IJCV)},
	doi = {10.1007/s11263-015-0816-y},
	volume={115},
	number={3},
	pages={211-252}
}

@misc{lin2015microsoft,
      title={Microsoft COCO: Common Objects in Context}, 
      author={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},
      year={2015},
      eprint={1405.0312},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{43022,
	title	= {Going Deeper with Convolutions},
	author	= {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
	year	= {2015},
	URL	= {http://arxiv.org/abs/1409.4842},
	booktitle	= {Computer Vision and Pattern Recognition (CVPR)}
}

@misc{he2015deep,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{szegedy2016inceptionv4,
      title={Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}, 
      author={Christian Szegedy and Sergey Ioffe and Vincent Vanhoucke and Alex Alemi},
      year={2016},
      eprint={1602.07261},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{10.1109/TKDE.2009.191,
	author = {Pan, Sinno Jialin and Yang, Qiang},
	title = {A Survey on Transfer Learning},
	year = {2010},
	issue_date = {October 2010},
	publisher = {IEEE Educational Activities Department},
	address = {USA},
	volume = {22},
	number = {10},
	issn = {1041-4347},
	url = {https://doi.org/10.1109/TKDE.2009.191},
	doi = {10.1109/TKDE.2009.191},
	abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
	journal = {IEEE Trans. on Knowl. and Data Eng.},
	month = oct,
	pages = {1345–1359},
	numpages = {15},
	keywords = {machine learning, Transfer learning, survey, machine learning, data mining., data mining., survey, Transfer learning}
}


@PhdThesis{Ruder2019Neural,
  title={Neural Transfer Learning for Natural Language Processing},
  author={Ruder, Sebastian},
  year={2019},
  school={National University of Ireland, Galway}
}

@misc{szegedy2015rethinking,
      title={Rethinking the Inception Architecture for Computer Vision}, 
      author={Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Zbigniew Wojna},
      year={2015},
      eprint={1512.00567},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{chollet2017xception,
      title={Xception: Deep Learning with Depthwise Separable Convolutions}, 
      author={François Chollet},
      year={2017},
      eprint={1610.02357},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zoph2018learning,
      title={Learning Transferable Architectures for Scalable Image Recognition}, 
      author={Barret Zoph and Vijay Vasudevan and Jonathon Shlens and Quoc V. Le},
      year={2018},
      eprint={1707.07012},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{kolesnikov2020big,
      title={Big Transfer (BiT): General Visual Representation Learning}, 
      author={Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Joan Puigcerver and Jessica Yung and Sylvain Gelly and Neil Houlsby},
      year={2020},
      eprint={1912.11370},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Chawla_2002,
   title={SMOTE: Synthetic Minority Over-sampling Technique},
   volume={16},
   ISSN={1076-9757},
   url={http://dx.doi.org/10.1613/jair.953},
   DOI={10.1613/jair.953},
   journal={Journal of Artificial Intelligence Research},
   publisher={AI Access Foundation},
   author={Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
   year={2002},
   month={Jun},
   pages={321–357}
}

@ARTICLE{4309137,
  author={D. L. {Wilson}},
  journal={IEEE Transactions on Systems, Man, and Cybernetics}, 
  title={Asymptotic Properties of Nearest Neighbor Rules Using Edited Data}, 
  year={1972},
  volume={SMC-2},
  number={3},
  pages={408-421},
  doi={10.1109/TSMC.1972.4309137}
}

@article{10.1145/1007730.1007735,
      author = {Batista, Gustavo E. A. P. A. and Prati, Ronaldo C. and Monard, Maria Carolina},
      title = {A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data},
      year = {2004},
      issue_date = {June 2004},
      publisher = {Association for Computing Machinery},
      address = {New York, NY, USA},
      volume = {6},
      number = {1},
      issn = {1931-0145},
      url = {https://doi.org/10.1145/1007730.1007735},
      doi = {10.1145/1007730.1007735},
      abstract = {There are several aspects that might influence the performance achieved by existing learning systems. It has been reported that one of these aspects is related to class imbalance in which examples in training data belonging to one class heavily outnumber the examples in the other class. In this situation, which is found in real world data describing an infrequent but important event, the learning system may have difficulties to learn the concept related to the minority class. In this work we perform a broad experimental evaluation involving ten methods, three of them proposed by the authors, to deal with the class imbalance problem in thirteen UCI data sets. Our experiments provide evidence that class imbalance does not systematically hinder the performance of learning systems. In fact, the problem seems to be related to learning with too few minority class examples in the presence of other complicating factors, such as class overlapping. Two of our proposed methods deal with these conditions directly, allying a known over-sampling method with data cleaning methods in order to produce better-defined class clusters. Our comparative experiments show that, in general, over-sampling methods provide more accurate results than under-sampling methods considering the area under the ROC curve (AUC). This result seems to contradict results previously published in the literature. Two of our proposed methods, Smote + Tomek and Smote + ENN, presented very good results for data sets with a small number of positive examples. Moreover, Random over-sampling, a very simple over-sampling method, is very competitive to more complex over-sampling methods. Since the over-sampling methods provided very good performance results, we also measured the syntactic complexity of the decision trees induced from over-sampled data. Our results show that these trees are usually more complex then the ones induced from original data. Random over-sampling usually produced the smallest increase in the mean number of induced rules and Smote + ENN the smallest increase in the mean number of conditions per rule, when compared among the investigated over-sampling methods.},
      journal = {SIGKDD Explor. Newsl.},
      month = jun,
      pages = {20–29},
      numpages = {10}
}